# Twitter-end-to-end-using-Airflow

This project highlights the implementation of an advanced data engineering pipeline, utilizing Apache Airflow and Python. By leveraging the Twitter API for data extraction and Python for comprehensive data transformation, we automated the entire workflow on Amazon EC2. Data was securely stored on Amazon S3, ensuring scalability and accessibility. This streamlined approach not only optimized operational processes but also facilitated data-driven insights crucial for strategic decision-making and gaining a competitive edge in the market

1️⃣ Data Extraction: Leveraging the flexibility of the Twitter API, I orchestrated data extraction workflows within Airflow's Directed Acyclic Graphs (DAGs). This streamlined approach ensured continuous, real-time data ingestion directly into our processing pipeline.

2️⃣ Data Transformation: Using Python, I implemented intricate data transformation logic within Airflow tasks. This included data cleaning, normalization, and feature engineering, optimizing datasets for subsequent analytical processes.

3️⃣ Workflow Automation: Deployed on Amazon EC2, Airflow served as our workflow orchestration engine. Its scheduler seamlessly managed task dependencies and execution, guaranteeing timely data processing and delivery.

4️⃣ Data Storage: Employing Amazon S3, we securely stored processed data outputs. This scalable object storage solution facilitated efficient data retrieval and integration, supporting future analytical endeavors and business intelligence initiatives.

The Architecture for this End to End project is as follows:-

![Twitter_ETL](https://github.com/Dhairshah/Twitter-end-to-end-using-Airflow/assets/88075307/3353f3b1-fe5a-4f06-a44a-d65e339fc512)
